import re
import emoji
import argparse
import pickle
import pandas as pd
import pycld2 as cld2

from typing import Tuple, Dict, List


def remove_whitespaces(text: str) -> str:
    """Returns text without white spaces."""
    return ' '.join(text.split())


def remove_emojis_and_whitespaces(text: str) -> str:
    """Returns text without emojis."""
    sample_wo_emoji = emoji.demojize(remove_whitespaces(text))
    # Rule out emoji string codes.
    sentence_without_emoji_patterns = re.sub(r':[^:]+:', '', sample_wo_emoji)
    # Rule out whitespaces possibly generated by
    # emoji removals.
    return ' '.join(sentence_without_emoji_patterns.split(' ')).strip()


def detect_via_cld2(text: str) -> Tuple[str, str, float, int]:
    """Returns detected language with related code and scores."""
    lang, code, percent, score = cld2.detect(text)[2][0]
    return lang.lower(), code.lower(), percent, score


def run_extraction(data_name: str) -> Dict[str, List[str]]:
    """Returns the review texts after extracting and grouped
    them based on their associated original language."""
    # Load & process data
    data = pd.read_csv(f"data/{data_name}")  # shape: (n, 1) with n reviews
    data = data.dropna(axis=0, how='any')  # drop null entries

    # Rule out emojis from texts
    reviews = data['text'].apply(lambda x: remove_emojis_and_whitespaces(x))
    reviews = reviews.dropna(axis=0, how='any')  # drop null entries (due to emoji removals)

    # Detect language_corpus per review
    languages = reviews.apply(lambda x: detect_via_cld2(x))
    languages = languages.apply(pd.Series)  # separate columns
    languages.columns = ['language', 'code', 'percent', 'score']

    # Concatenate texts and language_corpus into a Dataframe
    concat = pd.concat([reviews, languages], axis=1)

    # Rule out texts with unknown language.
    # Note: these texts could be treated in a different process.
    ruled_out = ['x_inherited', 'x_klingon', 'interlingue', 'interlingua']
    concat = concat[~concat['language'].isin(ruled_out)]
    concat = concat[concat['language'] != 'unknown']

    # Check language importance and set minimum 11 texts per language.
    # This minimum is useful to rule out possible false language detections
    # i.e. detections of uncommon language_corpus. Alternatively, the type of
    # language expected in detections is specified by the user prior to
    # performing translations, or when filtering texts associated to
    # false language detections).
    # Note: we may need extra (or alternative) processing methods for
    # ruling out wrong detections, other than limiting the number of
    # texts per language.
    counts = concat.groupby(by='language').count()
    languages_to_keep = counts[counts['text'] > 10].index.to_list()
    concat = concat[concat['language'].isin(languages_to_keep)]

    # Display number of texts per language, and percentage of texts
    # kept from original dataset after detections, i.e. percentage of
    # input text for which the language if effectively detected (without
    # error or false detections).
    # counts = counts[counts.index.isin(languages_to_keep)]
    kept = (100 * concat['language'].shape[0]) / data['text'].shape[0]  # expressed in %
    kept = 'kept: {:.2f}%'.format(kept)
    print(f"Data: {concat['language'].shape[0]} (initial: {data['text'].shape[0]}), " + kept)

    # Save texts grouped by detected language_corpus.
    texts_to_translate = {}
    for i, grp in enumerate(concat.groupby(by='language')):
        if grp[0].startswith('chines'):
            # Take into account 'chinese' and 'chineset'
            if 'chinese' not in texts_to_translate:
                texts_to_translate['chinese'] = []
                texts_to_translate['chinese'] += grp[1]['text'].to_list()
        else:
            texts_to_translate[grp[0]] = grp[1]['text'].to_list()

        msg = f"step {i+1}/{len(concat.groupby(by='language'))}"
        print(msg)
        yield msg

    name = data_name.split('.')[0] if '.csv' in data_name else data_name
    with open(f'processing/not_translated/{name}_not_translated.pickle', 'wb') as file:
        pickle.dump(texts_to_translate, file)

    yield texts_to_translate


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, default='reviews', help="Data to process.")
    args = parser.parse_args()

    run_extraction(data_name=str(args.data)+'.csv')
